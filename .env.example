# Environment
ENV=development
DEBUG=true

# API
API_HOST=0.0.0.0
API_PORT=8000

# LLM Provider (model-agnostic)
# Supported: openai, anthropic, ollama, vllm
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o
LLM_API_KEY=sk-your-api-key-here
LLM_BASE_URL=              # Custom endpoint for ollama/vllm (e.g. http://localhost:11434)
LLM_TEMPERATURE=0.0

# OpenAI API (backward compat â€” used as fallback if LLM_API_KEY is empty)
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# Guardrails Backend
# Options: langchain (LLM-as-judge), nemo (NeMo Guardrails), none (disabled)
GUARDRAILS_BACKEND=langchain

# Dynamic Rails (LLM-based topic classification)
DYNAMIC_RAILS_ENABLED=true
CLASSIFIER_MODEL=gpt-4o-mini  # Small model for classification (empty = gpt-4o-mini)
DYNAMIC_RAILS_MAX_RULES_PER_SESSION=50

# Redis (sessions)
REDIS_URL=redis://localhost:6379/0
REDIS_SESSION_TTL=3600

# Postgres (audit logs)
DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/guardrails_mvp

# Tool Proxy limits
TOOL_MAX_CALLS_PER_REQUEST=10
TOOL_RATE_LIMIT_PER_MIN=30
TOOL_LOOP_BREAKER_THRESHOLD=3
TOOL_TIMEOUT_SECONDS=15

# Guardrails (NeMo-specific)
GUARDRAILS_PROFILE=default
GUARDRAILS_MODE=enforce  # enforce | monitor
GUARDRAILS_MAX_REGEN=1

# Observability
LOG_LEVEL=INFO
METRICS_ENABLED=true
PROMETHEUS_PORT=9090

# HTTP/HTTPS Proxy (optional)
HTTP_PROXY=
HTTPS_PROXY=
